## 1. 如果只划分测试集和训练集，经验是**75%作为训练集**

sklearn中的train_test_split()默认这样划分

```Python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split()
```


## 2. 通常使用sklearn中的score方法计算结果的精度（正确预测比例）


## 3. KNN算法有两个重要参数：邻居个数和数据点之间距离的度量方法
- 在实践中，选择较小的邻居个数（例如3或5个）效果较好，sklearn中默认使用欧氏距离
- 构建KNN模型速度很快，若训练集很大（特征数多或样本数多），预测速度可能较慢
- 对于稀疏数据集（大多数特征值为0），KNN效果很不好


## 4.普通最小二乘法，即要求预测值和真实值均方误差最小，均方误差=总和(预测值-真实值)^2 / 样本数n


## 5.训练集和测试集之间的分数差异是过拟合的明显标志，e.g. 训练集0.95, 测试集0.61


## 6.线性回归防止过拟合可以采用岭回归（L2正则化）

from sklearn,linear_model import Ridge

ridge = Ridge().fit(X_train,y_train)

线性回归中的score为R^2

岭回归的结果，训练集分数可能低于线性回归，但测试集分数一般高于线性回归

默认岭回归的alpha = 1.0

岭回归惩罚了系数的L2范数或w的欧式长度

增大alpha会使得各个系数wi更加趋向0，从而提高泛化性能

可以比较alpha为10 和 0.1的scores

当岭回归alpha = 0时，即线性回归


## 7. 学习曲线：回归问题中，横轴为训练集大小，纵轴为score(R^2)，绘制训练集和测试集曲线


## 8. 如果有足够多的数据，正则化就不太重要了


## 9.线性回归防止过拟合可以采用Lasso回归（L1正则化）

因为Lasso会使得部分特征的系数（w）为0，所以相当于做特征筛选

只呈现重要的特征

from sklearn,linear_model import Lasso

ridge = Lasso().fit(X_train,y_train)

默认的alpha = 1

可能会欠拟合（训练集和测试集的score都不高）

我们此时应该减小alpha，增大max_iter（运行迭代的最大次数）

from sklearn,linear_model import Lasso

ridge = Lasso(alpha = 0.1，max_iter = 100000).fit(X_train,y_train)

如果alpha太小，则可能过拟合，即与线性回归效果相似


## 10. Lasso回归和岭回归一般会首选后者

如果特征很多，认为只有几个是重要的，则选择Lasso

如果为了模型的可解释性，选择Lasso（因为类似特征选择）


## 11.sklearn中逻辑回归和线性支持向量机都默认使用L2正则化

若换为L1正则化：

LogisticRegression（penalty = "l1").fit(X_train,y_train)

使用参数C表示正则化强度，C越小，正则化越强。

默认C=1

LogisticRegression(C=100).fit(X_train,y_train)

有时训练集测试集分数都很高，但是分数接近，可能是欠拟合

此时增大C，使正则化减弱看看结果分数


## 12.线性模型训练速度非常快，预测速度也很快，适合在大数据集上使用，也适合稀疏数据


## 13.朴素贝叶斯分类器速度更快（相比于线性分类器逻辑斯特回归和线性支持向量机），但是其泛化能力比线性分类器差


## 14.sklearn中提供三种朴素贝叶斯分类器，**后两个主要用于文本分类**
- GaussianNB课用于任意连续数据
- BernoulliNB假定输入数据为二分类数据
- MultinomialNB假定输入的数据为计数数据（即每个特征代表某个对象的整数计数，比如一个单词在句中出现的次数）


## 15.决策树也可用于回归任务，预测时基于每个节点的测试对树进行遍历，最终找到新数据点所属的叶节点

这一数据点的输出即为此叶节点中所有训练点的平均目标值。


## 16. 决策树可以采用预剪枝和后剪枝来防止过拟合

sklearn中只实现了预剪枝

预剪枝限制条件可以包括
- 限制树的最大深度
- 限制叶节点的最大数目
- 规定一个节点中数据点的最小数目防止继续划分


## 17.决策树graphviz可视化图中的samples给出该节点中的样本数，values给出每个类别的样本数


## 18.特征重要性指标可以看决策树的特征重要性，每个特征值介于0和1之间

tree.feature_importance_

且加和为1

0表示特征没用到

1表示完美预测目标值


## 19.决策树回归不能外推，也不能在训练数据范围之外进行预测


## 20. 决策树优点：
- 较小的树模型可视化容易，容易解释理解
- 算法不受数据缩放影响（因为每个特征单独处理），特征不需要预处理（归一化 标准化），特别是特征尺度大小相差大或者二元特征和连续特征同时存在时

决策树缺点：既使预剪枝，也经常过拟合，泛化能力差，所以大多数情况采用集成模型代替单棵决策树。


## 21.  集成方法有随机森林（random forest）和梯度提升树（gradient boosted decision tree）GBDT

随机森林中树的随机化方法有两种：

  1. 通过选择用于构造树的数据点
  1.1. 构造随机森林需要确定用于构造的树的个数
  1.2. 为了确保树与树之间的区别，对每棵树的数据进行自助采样
  1.3. 从样本数据中有放回的多次抽取（一个样本可能被抽取多次），抽取创建的新数据集要和原数据集大小相等（数据数量相同）

  2. 通过选择每次划分测试集的特征
  2.1. 在每个节点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试，而不是对每个节点都寻找最佳测试。
  2.2. 使用参数max_features, 如果max_features= n_features，则每次考虑所有特征，即第二种随机性没用上
    - 当max_features较大，随机森林中每棵树都会很相似（因为采用的特征基本相同）
    - 如果max_features较小，树的差异较大，为了很好的拟合数据，每棵树的深度都应该较大

RandomForestClassifier（n_estimators = 5,random_state = 2) # 五棵树的随机森林

随机森林比单独一棵树的过拟合都要小，实际应用中，我们会用很多树（通常几百上千),从而达到决策边界更平滑的效果

一般，随机森林给出的特征重要性比单科决策树给出的可靠。

多核CPU可以设置参数n_jobs = -1来使用计算机的所有内核计算

设置不同的随机状态（或者不设置random_state参数）可以彻底改变构建的模型

如果希望结果重现，固定random_state

对于高维稀疏数据（例如文本数据）随机森林常常表现不佳，使用线性模型更为合适

随机森林需要调节的重要参数有n_estimators和max_features，还包括预剪枝选项（如max_depth）
- n_estimators总是越大越好（时间内存允许的话）
- max_features决定每棵树的随机性大小，较小可以降低过拟合，一般使用默认值
- 分类时默认值为sqrt（n_features)
- 对于回归默认值是n_features


## 22.梯度提升回归树（梯度提升机）

梯度提升采用连续的方式构造树，每棵树都试图纠正前一棵树的错误。

梯度提升树背后的主要思想是合并许多简单的模型（在这个语境中叫做弱学习器），比如深度较小的树

每棵树只能对部分数据作出好的预测，所以添加更多的树可以不断迭代提高性能

除了**预剪枝**和**随机森林里的树的数量**之外，梯度提升树的另一个重要参数是**学习率（learning_rate)**

用于控制每棵树纠正前一棵树错误的强度，通过增大learning_rate或n_estimators都会增加模型的复杂度

**降低树的最大深度和学习率都能降低过拟合**

**GradientBoostingClassifier（random_state=0, max_depth=1, learning_rate=0.01)**

- 随机森林的n_estimators越大越好
- 梯度提升树的n_estimators提高，模型复杂，会导致过拟合
- 梯度提升树的max_depth通常设置的很小，一般不超过5

由于梯度提升和随机森林两种方法在类似的数据上表现的都很好，因此一种常用的方法是**先尝试随机森林，它的鲁棒性很好，如果随机森林效果很好，但是预测时间太长，再选择梯度提升**，梯度提升树**需要仔细调参，训练时间也长，也不适合高维稀疏数据**


## 23.对于SVM，将数据映射到更高维空间中有两种常用方法：
- 多项式核；在一定阶数内计算原始特征所有可能的多项式（例如features1**2， features2**5）
- 径向基函数（RBF) 核，也叫高斯核。它考虑所有阶数的所有可能的多项式，但阶数越高，特征的重要性越小

## 24.SVM调参
gamma参数，控制高斯核的宽度，它决定了点与点之间“靠近”是指多大的距离。C参数是正则化参数，与线性模型类似

它限制每个点的重要性

从小增大gamma（0.1-10），它认为点与点之间的距离不断增大，从决策边界平滑往不平滑过渡，模型越加复杂

这两个参数强烈相关，可以同时调节

C从小到大，决策边界越来越不平滑

默认情况下：C=1,gamma=1/n_features

SVM数据需要预处理（常用的是缩放到0~1之间）

常用的是（x-xmin）/（xmax-xmin）

SVM的缺点：需要预处理和小心调参，SVM模型很难检验，也难以解释



## 25.神经网络的非线性函数常用校正非线性（relu）或正切双曲线（tanh）

relu截断小于0的值

tanh在输入值小时接近-1，较大时接近1

有了这两种非线性函数，神经网络可以学习比线性模型复杂得多的函数

多层感知机（MLP )，也称为普通前馈神经网络，默认时，每层使用100个隐节点

默认的是relu

MLPClassifier（solver='lbfgs',random_state=0,hidden_layer_sizes=[10])

10层

如果是10层且每层10个隐节点

则hidden_layer_sizes=[10，10]

MLPClassifier中调节L2惩罚的参数是alpha（与线性回归模型相同），默认值很小（弱正则化）

控制神经网络的复杂度的方法有很多种，隐层的个数、每个隐层中的单元个数与正则化（alpha）

神经网络要求输入特征的变化范围相似，最理想的情况是均值为0，方差为1

我们必须对数据进行缩放达到这一要求

StandardScaler可以达到数据处理要求

迭代次数参数MLPClassifier（max_iter = 1000,random_state=0)

功能强大的神经网络经常需要很长的训练时间

神经网络的调参常用方法是，首先创建一个大到足以过拟合的网络，确保这个网络可以对任务进行学习

然后通过缩小网络或者增大alpha来增强正则化，从而提高泛化性能

如何学习模型或用来学习参数的算法，由solver参数设定

默认为'adam',在大多数情况下效果很好，但是对数据的缩放相当敏感（需要将数据缩放为均值为0，方差为1）

'lbfgs'的鲁棒性很好，大型数据集和大型模型上时间较长

更高级的'sgd'

初学者建议使用前两种

sklearn中有两个函数可用于获取分类器的不确定性估计：decision_function和predict_proba

大多数分类器都至少有其中一个函数，很多分类器这两个都有

predict_proba的结果是：每行的第一个元素是第一个类别的估计概率，第二个元素是第二个类别的估计概率

predict_proba输出的是概率，在0~1之间，两个类别的元素之和始终为1

decision_function每一列对应每个类别的确定度分数，分数越高类别的可能性越大



## 如何选择合适的模型：

模型 | 适合的数据集大小 | 高维数据 | 速度 | 数据缩放 | 参数 | 其它
--- | --- | --- | --- | --- | --- | ---
KNN | 小型数据集 | - | 构建模型速度很快，若训练集很大（特征数多or样本数多），预测速度可能较慢 | - | - | 容易解释
线性模型 | 非常大的数据集 | 适合 | 训练速度非常快，预测速度也很快 | - | - | 非常可靠的首选算法
朴素贝叶斯| 非常大的数据集 | 适合 | 比线性模型速度还快 | - | - | 只适用于分类问题，精度通常要低于线性模型
决策树 | - | - | 速度很快 | 不需要 | - | 可以可视化，容易解释
随机森林 | - | 不适用 | - | 不需要 | - | 几乎总是比单棵决策树表现好，鲁棒性好
梯度提升决策树 | - | - | 训练速度比随机森林慢，预测速度比随机森林快 | 不需要 | 比随机森林需要调的参数多 | 精度通常比随机森林略高，需要的**内存少**
SVM | 特征含义相似的中等大小的数据集 | - | - | 需要 | 对参数选取敏感 | -
神经网络 | 大型数据集 | - | 大型网络需要很长的训练时间 | 对数据缩放敏感 | 对参数选取敏感 | 可以构建非常复杂的模型

1. 面对数据集，先从简单的模型开始，比如线性回归、朴素贝叶斯、KNN，看看能得到的结果
2. 对数据理解加深后，可以考虑更复杂的模型，如随机森林、梯度提升决策树、SVM、神经网络

参考文献：[Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python)
